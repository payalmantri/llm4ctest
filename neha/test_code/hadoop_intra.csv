Dependency Taxonomy,Dependency,Checking Time,Violation handling,Impact of violations,Feedback,Configuration Parameter A,ComponentA,"Configuration Parameter B( ,C and more)",ComponentB,Related description,
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.refresh.url,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,mapreduce.shuffle.transferTo.allowed,mapred-default.xml,mapreduce.shuffle.transfer.buffer.size,mapred-default.xml,mapreduce.shuffle.transfer.buffer.size: This property is used only if mapreduce.shuffle.transferTo.allowed is set to false.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,rpc.metrics.percentiles.intervals,core-default.xml,rpc.metrics.quantile.enable,core-default.xml,rpc.metrics.quantile.enable: Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.msi.port,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.client.id,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.credential,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.access.token.provider,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.devicecode.clientapp.id,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,fs.adl.oauth2.access.token.provider.type,core-default.xml,fs.adl.oauth2.refresh.token,core-default.xml,"fs.adl.oauth2.access.token.provider.type: Defines Azure Active Directory OAuth2 access token provider type. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,spark.dynamicAllocation.enabled,spark.xml,spark.dynamicAllocation.minExecutors,spark.xml,"spark.dynamicAllocation.enabled: Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,spark.executor.logs.rolling.strategy,spark.xml,spark.executor.logs.rolling.maxSize,spark.xml,spark.executor.logs.rolling.strategy: Set the strategy of rolling of executor logs.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,spark.executor.logs.rolling.strategy,spark.xml,spark.executor.logs.rolling.time.interval,spark.xml,spark.executor.logs.rolling.strategy: Set the strategy of rolling of executor logs.,
Control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,fs.client.resolve.topology.enabled,core-default.xml,net.topology.node.switch.mapping.impl,core-default.xml,fs.client.resolve.topology.enabled: Whether the client machine will use the class specified by property net.topology.node.switch.mapping.impl to compute the network distance between itself and remote machines of the FileSystem.,
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,spark.memory.useLegacyMode,spark.xml,spark.storage.unrollFraction,spark.xml,"spark.memory.useLegacyMode: Whether to enable the legacy memory management mode used in Spark 1.5 and before. The following deprecated memory fraction configurations are not read unless this is enabled: spark.shuffle.memoryFraction, spark.storage.memoryFraction, spark.storage.unrollFraction.",
control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,net.topology.node.switch.mapping.impl,core-default.xml,net.topology.table.file.name,core-default.xml,"Whether the client machine will use the class specified by property net.topology.node.switch.mapping.impl to compute the network distance between itself and remote machines of the FileSystem. If org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file needs to be specified in net.topology.script.file.name. ",
control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,dfs.datanode.fsdataset.volume.choosing.policy,hdfs-default.xml,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,hdfs-default.xml,Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.,
control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,dfs.block.replicator.classname,hdfs-default.xml,dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction,hdfs-default.xml,Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.,
control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,dfs.datanode.fsdataset.volume.choosing.policy,hdfs-default.xml,dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,hdfs-default.xml,Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.,
control Dependency,"if A is SomeValue, B will work, class based control",no check,N/A,Usability,None,yarn.resourcemanager.configuration.provider-class,yarn-default.xml,yarn.resourcemanager.configuration.file-system-based-store,yarn-default.xml,The value specifies the file system (e.g. HDFS) path where ResourceManager loads configuration if yarn.resourcemanager.configuration.provider-class is set to org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider.,
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.nodemanager.node-labels.provider,yarn-default.xml,yarn.nodemanager.node-labels.provider.fetch-interval-ms,yarn-default.xml,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"", ""Script"" or the configured class extends AbstractNodeLabelsProvider, then periodically node labels are retrieved from the node labels provider. ",
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.scheduler.configuration.store.class,yarn-default.xml,yarn.scheduler.configuration.store.max-logs,yarn-default.xml,"The max number of configuration change log entries kept in config store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"" or ""zk"".",
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.scheduler.configuration.store.class,yarn-default.xml,yarn.scheduler.configuration.leveldb-store.path,yarn-default.xml," The storage path for LevelDB implementation of configuration store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"". ",
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.resourcemanager.scheduler.monitor.enable,yarn-default.xml,yarn.resourcemanager.scheduler.monitor.policies,yarn-default.xml,Enable a set of periodic monitors (specified in yarn.resourcemanager.scheduler.monitor.policies) that affect the scheduler.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,dfs.namenode.lifeline.rpc-address,hdfs-default.xml,dfs.namenode.lifeline.handler.count,hdfs-default.xml,dfs.namenode.lifeline.handler.count: This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.,
control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.nodemanager.resource.cpu-vcores,yarn-default.xml,yarn.nodemanager.resource.pcores-vcores-multiplier,yarn-default.xml,This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.nodemanager.node-labels.provider,yarn-default.xml,yarn.nodemanager.node-labels.provider.fetch-timeout-ms,yarn-default.xml,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""Script"" then this configuration provides the timeout period after which it will interrupt the script which queries the Node labels.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.nodemanager.resource.memory-mb,yarn-default.xml,yarn.nodemanager.resource.system-reserved-memory-mb,yarn-default.xml,This configuration is only used if yarn.nodemanager.resource.detect-hardware-capabilities is set to true and yarn.nodemanager.resource.memory-mb is -1.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.nodemanager.node-labels.provider,yarn-default.xml,yarn.nodemanager.node-labels.provider.configured-node-partition,yarn-default.xml,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"" then ConfigurationNodeLabelsProvider fetches the partition label from this parameter.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.ssl.keystore.password,core-default.xml,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,core-default.xml,"If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,spark.deploy.recoveryMode,spark.xml,spark.deploy.zookeeper.url,spark.xml,"spark.deploy.zookeeper.url: When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper URL to connect to.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,hbase.offpeak.start.hour,hbase-default.xml,hbase.hstore.compaction.ratio.offpeak,hbase-default.xml,Only applies if hbase.offpeak.start.hour and hbase.offpeak.end.hour are also enabled.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,hbase.offpeak.end.hour,hbase-default.xml,hbase.hstore.compaction.ratio.offpeak,hbase-default.xml,Only applies if hbase.offpeak.start.hour and hbase.offpeak.end.hour are also enabled.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,yarn.scheduler.configuration.store.class,yarn-default.xml,yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,yarn-default.xml,"The compaction interval for LevelDB configuration store in secs, when yarn.scheduler.configuration.store.class is configured to be ""leveldb""",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,dfs.namenode.lifeline.rpc-address,hdfs-default.xml,dfs.namenode.lifeline.handler.ratio,hdfs-default.xml,"If this property is defined, then it overrides the behavior of dfs.namenode.lifeline.handler.ratio.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,mapreduce.jobhistory.loadedtasks.cache.size,mapred-default.xml,mapreduce.jobhistory.loadedjobs.cache.size,mapred-default.xml,If this value is empty or nonpositive then the cache reverts to using the property mapreduce.jobhistory.loadedjobs.cache.size as a job cache size.,
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,hbase.master.infoserver.redirect,hbase-default.xml,hbase.master.info.port,hbase-default.xml,hbase.master.infoserver.redirect: Whether or not the Master listens to the Master web UI port (hbase.master.info.port),
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,fs.s3a.multipart.purge,core-default.xml,fs.s3a.multipart.purge.age,core-default.xml,fs.s3a.multipart.purge: True if you want to purge existing multipart uploads that may not have been completed/aborted correctly. The corresponding purge age is defined in fs.s3a.multipart.purge.age.,
Control dependency,"If A is not true, B will work",Init Time,Logging only,Usability,Complete,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.client.domain.socket.data.traffic,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control dependency,"If A is not true, B will work",no check,N/A,Usability,None,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.client.read.shortcircuit.skip.checksum,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control dependency,"If A is not true, B will work",no check,N/A,Usability,None,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.client.read.shortcircuit.buffer.size,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control dependency,"If A is not true, B will work",no check,N/A,Usability,None,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.client.read.shortcircuit.streams.cache.size,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control dependency,"If A is not true, B will work",no check,N/A,Usability,None,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.client.read.shortcircuit.streams.cache.expiry.ms,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control dependency,"If A is not true, B will work",no check,N/A,Usability,None,hbase.defaults.for.version.skip,hbase-default.xml,hbase.defaults.for.version,hbase-default.xml,Set to true to skip the 'hbase.defaults.for.version' check.,
Control dependency,"If A is set to SomeValue, B will work",no check,N/A,Usability,None,hadoop.security.saslproperties.resolver.class,core-default.xml,hadoop.rpc.protection,core-default.xml,hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side. ,
Control dependency,"If A is set to SomeValue, B will work",no check,N/A,Usability,None,hadoop.security.saslproperties.resolver.class,core-default.xml,hadoop.rpc.protection.non-whitelist,core-default.xml,hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side. ,
Control dependency,"If A is not set, B will work",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.bind.password,core-default.xml,hadoop.security.group.mapping.ldap.bind.password.file,core-default.xml,"If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file.",
Control dependency,"If A is not set, B will work",no check,N/A,Usability,None,dfs.namenode.lifeline.handler.count,hdfs-default.xml,dfs.namenode.lifeline.handler.ratio,hdfs-default.xml,"If this property is defined, then it overrides the behavior of dfs.namenode.lifeline.handler.ratio.",
Control Dependency,"If A is set to SomeValue, B will work",no check,N/A,Usability,None,yarn.resourcemanager.store.class,yarn-default.xml,yarn.resourcemanager.zk-state-store.parent-path,yarn-default.xml,This configuration has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default),
Control Dependency,"If A is set to SomeValue, B will work",no check,N/A,Usability,None,yarn.resourcemanager.store.class,yarn-default.xml,yarn.resourcemanager.fs.state-store.uri,yarn-default.xml,This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore as the value for yarn.resourcemanager.store.class,
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,alluxio.security.authentication.type,Alluxio.xml,alluxio.security.login.username,Alluxio.xml,"alluxio.security.login.username: When alluxio.security.authentication.type is set to SIMPLE or CUSTOM, user application uses this property to indicate the user requesting Alluxio service.",
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,alluxio.security.authentication.type,Alluxio.xml,alluxio.security.authentication.custom.provider.class,Alluxio.xml,"The class to provide customized authentication implementation, when alluxio.security.authentication.type is set to CUSTOM.",
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,alluxio.security.authentication.type,Alluxio.xml,alluxio.security.login.impersonation.username,Alluxio.xml,"When alluxio.security.authentication.type is set to SIMPLE or CUSTOM, user application uses this property to indicate the IMPERSONATED user requesting Alluxio service.",
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,electionType,ZooKeeper.xml,cnxTimeout,ZooKeeper.xml,Only applicable if you are using electionAlg 3,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.balancer.keytab.enabled,hdfs-default.xml,dfs.balancer.keytab.file,hdfs-default.xml,Keytab based login can be enabled with dfs.balancer.keytab.enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,rpc.metrics.quantile.enable,core-default.xml,rpc.metrics.percentiles.intervals,core-default.xml,rpc.metrics.quantile.enable: Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,fs.azure.secure.mode,core-default.xml,fs.azure.local.sas.key.mode,core-default.xml,"Works in conjuction with fs.azure.secure.mode. Setting this config to true results in fs.azure.NativeAzureFileSystem using the local SAS key generation where the SAS keys are generating in the same process as fs.azure.NativeAzureFileSystem. If fs.azure.secure.mode flag is set to false, this flag has no effect. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,fs.azure.authorization,core-default.xml,fs.azure.enable.spnego.token.cache,core-default.xml,"Works in conjuction with fs.azure.secure.mode. Setting this config to true results in fs.azure.NativeAzureFileSystem using the local SAS key generation where the SAS keys are generating in the same process as fs.azure.NativeAzureFileSystem. If fs.azure.secure.mode flag is set to false, this flag has no effect. ",
Control Dependency,"If A is true, B will work. IR code????",no check,N/A,Usability,None,hadoop.security.groups.cache.background.reload,core-default.xml,hadoop.security.groups.cache.background.reload.threads,core-default.xml,Only relevant if hadoop.security.groups.cache.background.reload is true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,fs.s3a.fast.upload,core-default.xml,fs.s3a.fast.upload.buffer,core-default.xml,The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). This configuration option has no effect if fs.s3a.fast.upload is false.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,ipc.client.ping,core-default.xml,ipc.ping.interval,core-default.xml,"Timeout on waiting response from server, in milliseconds. The client will send ping when the interval is passed without receiving bytes, if ipc.client.ping is set to true. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.datanode.sync.behind.writes,hdfs-default.xml,dfs.datanode.sync.behind.writes.in.background,hdfs-default.xml,"If set to true, then sync_file_range() system call will occur asynchronously. This property is only valid when the property dfs.datanode.sync.behind.writes is true. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.balancer.keytab.enabled,hdfs-default.xml,dfs.balancer.address,hdfs-default.xml,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.webhdfs.rest-csrf.enabled,hdfs-default.xml,dfs.webhdfs.rest-csrf.methods-to-ignore,hdfs-default.xml,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.client.block.write.replace-datanode-on-failure.enable,hdfs-default.xml,dfs.client.block.write.replace-datanode-on-failure.policy,hdfs-default.xml,"A comma-separated list of HTTP methods that do not require HTTP requests to include a custom header when protection against cross-site request forgery
    (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to
    true.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.webhdfs.rest-csrf.enabled,hdfs-default.xml,dfs.webhdfs.rest-csrf.custom-header,hdfs-default.xml,This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,yarn-default.xml,yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,yarn-default.xml,This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.linux-container-executor.cgroups.mount,yarn-default.xml,yarn.nodemanager.linux-container-executor.cgroups.mount-path,yarn-default.xml,The UNIX user that containers will run as when Linux-container-executor is used in nonsecure mode (a use case for this is using cgroups) if the yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is set to true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.resource.detect-hardware-capabilities,yarn-default.xml,yarn.nodemanager.resource.system-reserved-memory-mb,yarn-default.xml,Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.resource.detect-hardware-capabilities,yarn-default.xml,yarn.nodemanager.resource.count-logical-processors-as-cores,yarn-default.xml,This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.resource.detect-hardware-capabilities,yarn-default.xml,yarn.nodemanager.resource.pcores-vcores-multiplier,yarn-default.xml,yarn.nodemanager.resource.pcores-vcores-multiplier: Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.task.profile,mapred-default.xml,mapreduce.task.profile.reduces,mapred-default.xml,To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted. ,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.task.profile,mapred-default.xml,mapreduce.task.profile.maps,mapred-default.xml, To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.job.classloader,mapred-default.xml,mapreduce.job.classloader.system.classes,mapred-default.xml,"indicate whether to load a class from the system classpath, instead from the user-supplied JARs, when mapreduce.job.classloader is enabled.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hbase.coprocessor.enabled,hbase-default.xml,hbase.coprocessor.user.enabled,hbase-default.xml,"If ""hbase.coprocessor.enabled"" is 'false' this setting has no effect. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hbase.rest.csrf.enabled,hbase-default.xml,hbase.rest-csrf.browser-useragents-regex,hbase-default.xml,used to match against an HTTP request's User-Agent header when protection against cross-site request forgery (CSRF) is enabled for REST server by setting hbase.rest.csrf.enabled to true.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.task.reaper.enabled,Spark.xml,spark.task.reaper.pollingInterval,Spark.xml,"When spark.task.reaper.enabled = true, this setting controls the frequency at which executors will poll the status of killed tasks.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.eventLog.enabled,Spark.xml,spark.eventLog.compress,Spark.xml,"Whether to compress logged events, if spark.eventLog.enabled is true.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.task.reaper.enabled,Spark.xml,spark.task.reaper.threadDump,Spark.xml,"When spark.task.reaper.enabled = true, this setting controls whether task thread dumps are logged during periodic polling of killed tasks",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.memory.useLegacyMode,Spark.xml,spark.storage.memoryFraction,Spark.xml,This is read only if spark.memory.useLegacyMode is enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.memory.useLegacyMode,Spark.xml,spark.shuffle.memoryFraction,Spark.xml,This is read only if spark.memory.useLegacyMode is enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.eventLog.enabled,Spark.xml,spark.eventLog.dir,Spark.xml,"Base directory in which Spark events are logged, if spark.eventLog.enabled is true.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.eventLog.enabled,Spark.xml,spark.eventLog.logBlockUpdates.enabled,Spark.xml,"Whether to log events for every block update, if spark.eventLog.enabled is true",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.task.reaper.enabled,Spark.xml,spark.task.reaper.killTimeout,Spark.xml,"When spark.task.reaper.enabled = true, this setting specifies a timeout after which the executor JVM will kill itself if a killed task has not stopped running.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.ssl,core-default.xml,hadoop.security.group.mapping.ldap.ssl.truststore,core-default.xml,hadoop.security.group.mapping.ldap.ssl: Whether or not to use SSL when connecting to the LDAP server.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.ssl,core-default.xml,hadoop.security.group.mapping.ldap.ssl.keystore.password,core-default.xml,hadoop.security.group.mapping.ldap.ssl: Whether or not to use SSL when connecting to the LDAP server.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.ssl,core-default.xml,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,core-default.xml,hadoop.security.group.mapping.ldap.ssl: Whether or not to use SSL when connecting to the LDAP server.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.registry.secure,core-default.xml,hadoop.registry.jaas.context,core-default.xml,Key to define the JAAS context. Used in secure mode ,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.ssl,core-default.xml,hadoop.security.group.mapping.ldap.ssl.keystore,core-default.xml,hadoop.security.group.mapping.ldap.ssl: Whether or not to use SSL when connecting to the LDAP server.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.security.dns.log-slow-lookups.enabled,core-default.xml,hadoop.security.dns.log-slow-lookups.threshold.ms,core-default.xml,"If slow lookup logging is enabled, this threshold is used to decide if a lookup is considered slow enough to be logged.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,hadoop.ssl.enabled,core-default.xml,hadoop.ssl.enabled.protocols,core-default.xml,hadoop.ssl.enabled.protocols: The supported SSL protocols. ,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.namenode.replication.considerLoad,hdfs-default.xml,dfs.namenode.replication.considerLoad.factor,hdfs-default.xml,"The factor by which a node's load can exceed the average before being rejected for writes, only if considerLoad is true. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,dfs.encrypt.data.transfer.cipher.suites,hdfs-default.xml,dfs.encrypt.data.transfer.cipher.key.bitlength,hdfs-default.xml,Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.resourcemanager.ha.enabled,yarn-default.xml,yarn.client.failover-sleep-max-ms,yarn-default.xml,"When HA is enabled, the maximum sleep time (in milliseconds) between failovers.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.resourcemanager.ha.enabled,yarn-default.xml,yarn.resourcemanager.ha.automatic-failover.enabled,yarn-default.xml,"Enable automatic failover. By default, it is enabled only when HA is enabled",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.resourcemanager.ha.enabled,yarn-default.xml,yarn.client.failover-max-attempts,yarn-default.xml,"When HA is enabled, the max number of times FailoverProxyProvider should attempt failover.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.resourcemanager.ha.enabled,yarn-default.xml,yarn.client.failover-sleep-base-ms,yarn-default.xml,"When HA is enabled, the sleep base (in milliseconds) to be used for calculating the exponential delay between failovers.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.acl.enable,yarn-default.xml,yarn.acl.reservation-enable,yarn-default.xml,Are reservation acls enabled. Are acls enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.nodemanager.recovery.enabled,yarn-default.xml,yarn.nodemanager.recovery.dir,yarn-default.xml,The local filesystem directory in which the node manager will store state when recovery is enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,yarn.resourcemanager.ha.enabled,yarn-default.xml,yarn.resourcemanager.ha.automatic-failover.embedded,yarn-default.xml,yarn.resourcemanager.ha.automatic-failover.embedded: Enable embedded automatic failover.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.map.output.compress,mapred-default.xml,mapreduce.map.output.compress.codec,mapred-default.xml,"mapreduce.map.output.compress.codec: If the map outputs are compressed, how should they be compressed?",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.output.fileoutputformat.compress,mapred-default.xml,mapreduce.output.fileoutputformat.compress.type,mapred-default.xml,"mapreduce.output.fileoutputformat.compress.type: If the job outputs are to compressed as SequenceFiles, how should they be compressed?",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.ifile.readahead,mapred-default.xml,mapreduce.ifile.readahead.bytes,mapred-default.xml,mapreduce.ifile.readahead.bytes: Configuration key to set the IFile readahead length in bytes.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.output.fileoutputformat.compress,mapred-default.xml,mapreduce.output.fileoutputformat.compress.codec,mapred-default.xml,"If the job outputs are compressed, how should they be compressed? ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.streaming.backpressure.rateEstimator,Spark.xml,spark.streaming.backpressure.pid.derived,Spark.xml,Enable profiling in Python worker,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.streaming.backpressure.rateEstimator,Spark.xml,spark.streaming.backpressure.pid.minRate,Spark.xml,This is the initial maximum receiving rate at which each receiver will receive data for the first batch when the backpressure mechanism is enabled. ,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.dynamicAllocation.enabled,Spark.xml,spark.dynamicAllocation.schedulerBacklogTimeout,Spark.xml,"If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description. ",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.cleaner.referenceTracking,Spark.xml,spark.cleaner.referenceTracking.blocking,Spark.xml,spark.cleaner.referenceTracking: Enables or disables context cleaning.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.dynamicAllocation.enabled,Spark.xml,spark.dynamicAllocation.cachedExecutorIdleTimeout,Spark.xml,"If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.dynamicAllocation.enabled,Spark.xml,spark.dynamicAllocation.executorIdleTimeout,Spark.xml,"If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed.",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.cleaner.referenceTracking,Spark.xml,spark.cleaner.referenceTracking.blocking.shuffle,Spark.xml,spark.cleaner.referenceTracking: Enables or disables context cleaning.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,spark.speculation,Spark.xml,spark.speculation.interval,Spark.xml,"spark.speculation: If set to ""true"", performs speculative execution of tasks.",
Control Dependency,"If A is SomeValue, B will work.",no check,N/A,Usability,None,hbase.security.authentication,hbase-default.xml,hbase.auth.key.update.interval,hbase-default.xml,The update interval for master key for authentication tokens in servers in milliseconds. Only used when HBase security is enabled.,
Control Dependency,"If A is SomeValue, B will work.",no check,N/A,Usability,None,hbase.security.authentication,hbase-default.xml,hbase.auth.token.max.lifetime,hbase-default.xml,The maximum lifetime in milliseconds after which an authentication token expires. Only used when HBase security is enabled.,
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.job.ubertask.enable,mapred-default.xml,mapreduce.job.ubertask.maxmaps,mapred-default.xml,"mapreduce.job.ubertask.enable: Whether to enable the small-jobs ""ubertask"" optimization,",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.job.ubertask.enable,mapred-default.xml,mapreduce.job.ubertask.maxbytes,mapred-default.xml,"mapreduce.job.ubertask.enable: Whether to enable the small-jobs ""ubertask"" optimization,",
Control Dependency,"If A is true, B will work.",no check,N/A,Usability,None,mapreduce.job.ubertask.enable,mapred-default.xml,mapreduce.job.ubertask.maxreduces,mapred-default.xml,"mapreduce.job.ubertask.enable: Whether to enable the small-jobs ""ubertask"" optimization,",
Control Dependency,"if A is true, B will work.",Runtime,Runtime Exception,Runtime Failure,Complete,spark.memory.offHeap.enabled,Spark.xml,spark.memory.offHeap.size,Spark.xml,"spark.memory.offHeap.enabled: If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory use is enabled, then spark.memory.offHeap.size must be positive. ",
Control Dependency,"if A is true, B will work.",no check,N/A,Usability,None,yarn.timeline-service.recovery.enabled,yarn-default.xml,yarn.timeline-service.state-store-class,yarn-default.xml,"Enable timeline server to recover state after starting. If true, then yarn.timeline-service.state-store-class must be specified. ",
Control Dependency,"If A is SomeValue, B will work",no check,N/A,Usability,None,fs.s3n.multipart.uploads.enabled,core-default.xml,fs.s3n.multipart.uploads.block.size,core-default.xml,"fs.s3n.multipart.uploads.enabled: Setting this property to true enables multiple uploads to native S3 filesystem. When uploading a file, it is split into blocks if the size is larger than fs.s3n.multipart.uploads.block.size.",
Control Dependency,"If A is true, B will work. ",no check,N/A,Usability,None,dfs.xframe.enabled,hdfs-default.xml,dfs.xframe.value,hdfs-default.xml,"dfs.xframe.enabled: If true, then enables protection against clickjacking by returning X_FRAME_OPTIONS header value set to SAMEORIGIN.",
Control Dependency,"if A is SomeValue, B will work",no check,N/A,Usability,None,dfs.namenode.servicerpc-address,hdfs-default.xml,dfs.namenode.service.handler.count,hdfs-default.xml,dfs.namenode.service.handler.count: The number of Namenode RPC server threads that listen to requests from DataNodes and from all other non-client nodes. dfs.namenode.service.handler.count will be valid only if dfs.namenode.servicerpc-address is configured. ,
Control Dependency,"If A != 0, B will work",no check,N/A,Usability,None,dfs.datanode.max.locked.memory,hdfs-default.xml,dfs.cachereport.intervalMsec,hdfs-default.xml,This configuration (dfs.cachereport.intervalMsec) has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default),
Control Dependency,"If A is not true, B will work",Init Time,Logging only,Usability,Complete,dfs.client.read.shortcircuit,hdfs-default.xml,dfs.domain.socket.path,hdfs-default.xml,dfs.client.read.shortcircuit: This configuration parameter turns on short-circuit local reads.,
Control Dependency,"if A is true, B will work.",no check,N/A,Usability,None,alluxio.worker.file.persist.rate.limit.enabled,Alluxio.xml,alluxio.worker.file.persist.rate.limit,Alluxio.xml,alluxio.worker.file.persist.rate.limit.enabled: Whether to enable rate limiting when performing asynchronous persistence.,
Control Dependency,"If A is SomeValue, B will work.",no check,N/A,Usability,None,yarn.node-labels.configuration-type,yarn-default.xml,yarn.resourcemanager.node-labels.provider,yarn-default.xml,"yarn.resourcemanager.node-labels.provider: When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter.",
Value Relationship (Numeric),A <= B ,Runtime,Correction without logging,Usability,None,mapreduce.job.end-notification.retry.attempts,mapred-default.xml,mapreduce.job.end-notification.max.attempts,mapred-default.xml,This is capped by mapreduce.job.end-notification.max.attempts,
Value Relationship (Numeric),A <= B ,Runtime,Correction without logging,Usability,None,mapreduce.job.end-notification.retry.interval,mapred-default.xml,mapreduce.job.end-notification.max.retry.interval,mapred-default.xml,This is capped by mapreduce.job.end-notification.max.retry.interval,
Value Relationship (Numeric),A <= B ,Init Time,Init Time Exception,Startup Failure,Complete,alluxio.master.worker.threads.min,Alluxio.xml,alluxio.master.worker.threads.max,Alluxio.xml,The minimum number of threads used to handle incoming RPC requests to master.,
Value Relationship (Numeric),A <= B ,Runtime,Runtime Exception,Runtime Failure,Complete,spark.kryoserializer.buffer,Spark.xml,spark.kryoserializer.buffer.max,Spark.xml,"Maximum allowable size of Kryo serialization buffer,",
Value Relationship (Numeric),A <= B ,Init Time,Init Time Exception,Startup Failure,Complete,spark.executor.heartbeatInterval,Spark.xml,spark.network.timeout,Spark.xml,spark.executor.heartbeatInterval should be significantly less than spark.network.timeout,
Value Relationship (Numeric),A <= B,Init Time,Correction without logging,Usability,None,yarn.resourcemanager.state-store.max-completed-applications,yarn-default.xml,yarn.resourcemanager.max-completed-applications,yarn-default.xml,"The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. ",
Value Relationship (Numeric),A <= B,Init Time,Correction without logging,Usability,None,fs.trash.checkpoint.interval,core-default.xml,fs.trash.interval,core-default.xml,"Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.",
Value Relationship (Numeric),A >= B ,Init Time,Correction with logging,Usability,Complete,dfs.datanode.lifeline.interval.seconds,hdfs-default.xml,dfs.heartbeat.interval,hdfs-default.xml,The value must be greater than the value of dfs.heartbeat.interval,
Value Relationship (Numeric),A >= B ,Init Time,Init Time Exception,Startup Failure,Complete,yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,yarn-default.xml,yarn.nm.liveness-monitor.expiry-interval-ms,yarn-default.xml, It is expected to be set to a value much larger than yarn.nm.liveness-monitor.expiry-interval-ms. ,
Value Relationship (Numeric),A >= B ,Init Time,Init Time Exception,Startup Failure,Complete,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,yarn-default.xml,yarn.nm.liveness-monitor.expiry-interval-ms,yarn-default.xml,It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined. ,
Value Relationship (Numeric),A <= B,Init Time,Init Time Exception,Startup Failure,Complete,dfs.namenode.replication.min,hdfs-default.xml,dfs.replication,hdfs-default.xml,Minimal block replication.,
Value Relationship (Numeric),A >= B ,Init Time,Init Time Exception,Startup Failure,Complete,dfs.replication.max,hdfs-default.xml,dfs.replication,hdfs-default.xml,Maximal block replication.,
Value Relationship (Numeric),A >= B,Init Time,Correction with logging,Usability,Partial,dfs.namenode.stale.datanode.interval,hdfs-default.xml,dfs.namenode.heartbeat.recheck-interval,hdfs-default.xml,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond. ",
Value Relationship (Numeric),A <= B * 2 + 10 * 1000 * C,Init Time,Logging only,Usability,Complete,dfs.namenode.stale.datanode.interval,hdfs-default.xml,dfs.namenode.heartbeat.recheck-interval; dfs.namenode.heartbeat,hdfs-default.xml,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond. ",
Value Relationship (Numeric),A >= B * C * 1000,Init Time,Correction with logging,Usability,Complete,dfs.namenode.stale.datanode.interval,hdfs-default.xml,dfs.namenode.stale.datanode.minimum.interval; dfs.namenode.heartbeat,hdfs-default.xml,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond. ",
Value Relationship (Numeric),A >= B + C,Init Time,Correction with logging,Usability,Complete,alluxio.underfs.s3.threads.max,Alluxio.xml,alluxio.underfs.s3.admin.threads.max; alluxio.underfs.s3.upload.threads.max,Alluxio.xml,The maximum number of threads to use for communicating with S3 and the maximum number of concurrent connections to S3.,
Value Relationship (Numeric),A >= B,Runtime,Correction with logging,Usability,Inadequate,hadoop.registry.zk.retry.ceiling.ms,core-default.xml,hadoop.registry.zk.retry.interval.ms,core-default.xml,"This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period ",
Value Relationship (Numeric),A >= B,Init Time,Correction with logging,Usability,Complete,hbase.client.pause.cqtbe,hbase-default.xml,hbase.client.pause,hbase-default.xml,Set this property to a higher value than hbase.client.pause if you observe frequent CQTBE from the same RegionServer and the call queue there keeps full,
Value Relationship (Numeric),A <= B,Init Time,Correction without logging,Usability,None,dfs.namenode.checkpoint.check.period,hdfs-default.xml,dfs.namenode.checkpoint.period,hdfs-default.xml,dfs.namenode.checkpoint.check.period should be smaller than dfs.namenode.checkpoint.period,
Value Relationship (Numeric),A <= B,Init Time,Init Time Exception,Startup Failure,Complete,dfs.namenode.maintenance.replication.min,hdfs-default.xml,dfs.replication,hdfs-default.xml,Minimal live block replication in existence of maintenance mode.,
Value Relationship (Numeric),A <= B ,Runtime,Correction without logging,Usability,None,file.bytes-per-checksum,core-default.xml,file.stream-buffer-size,core-default.xml,The number of bytes per checksum. Must not be larger than file.stream-buffer-size,
Value Relationship (Numeric),A <= B ,Init Time,Init Time Exception,Startup Failure,Partial,yarn.scheduler.minimum-allocation-vcores,yarn-default.xml,yarn.nodemanager.resource.cpu-vcores,yarn-default.xml,"yarn.nodemanager.resource.memory-mb: Amount of physical memory, in MB, that can be allocated for containers.",
Value Relationship (Numeric),A <= B ,Init Time,Init Time Exception,Startup Failure,Partial,yarn.scheduler.minimum-allocation-mb,yarn-default.xml,yarn.nodemanager.resource.memory-mb,yarn-default.xml,"yarn.nodemanager.resource.memory-mb: Amount of physical memory, in MB, that can be allocated for containers.",
Value Relationship (Numeric),A <= B,Init Time,Init Time Exception,Startup Failure,Complete,alluxio.worker.network.netty.watermark.low,Alluxio.xml,alluxio.worker.network.netty.watermark.high,Alluxio.xml,"alluxio.worker.network.netty.watermark.high: Determines how many bytes can be in the write queue before switching to non-writable.
alluxio.worker.network.netty.watermark.low: Once the high watermark limit is reached, the queue must be flushed down to the low watermark.",
Value Relationship (Numeric),A > B,Runtime,Runtime Exception,Runtime Failure,Inadequate,alluxio.worker.block.threads.max,Alluxio.xml,alluxio.user.block.worker.client.threads,Alluxio.xml,This value (alluxio.worker.block.threads.max) should be greater than the sum of `alluxio.user.block.worker.client.threads` across concurrent Alluxio clients.,
Value Relationship (Numeric),"A >= B, no pattern",no check,N/A,Service Degradation,None,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,yarn-default.xml,yarn.resourcemanager.rm.container-allocation.expiry-interval-ms,yarn-default.xml,It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined. ,
Value Relationship (Numeric),"A <= B, no pattern",no check,N/A,Service Degradation,None,dfs.balancer.getBlocks.min-block-size,hdfs-default.xml,dfs.balancer.getBlocks.size,hdfs-default.xml,Minimum block threshold size in bytes to ignore when fetching a source's block list.,
Value Relationship (Numeric),"A <= B , no pattern",no check,N/A,Service Degradation,None,mapreduce.job.maxtaskfailures.per.tracker,mapred-default.xml,mapreduce.reduce.maxattempts,mapred-default.xml,It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node. ,
Value Relationship (Numeric),"A <= B  , no pattern",no check,N/A,Service Degradation,None,mapreduce.job.maxtaskfailures.per.tracker,mapred-default.xml,mapreduce.map.maxattempts,mapred-default.xml,It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node. ,
Value Relationship (Numeric),"A <= B , no pattern",no check,N/A,Service Degradation,None,dfs.namenode.replication.min,hdfs-default.xml,dfs.namenode.safemode.replication.min,hdfs-default.xml,dfs.namenode.safemode.replication.min This is an expert level setting. Setting this lower than the dfs.namenode.replication.min is not recommend and/or dangerous for production setups.,
Value Relationship (Numeric),"A >= B , no pattern",no check,N/A,Service Degradation,None,dfs.namenode.replication.max-streams-hard-limit,hdfs-default.xml,dfs.namenode.replication.max-streams,hdfs-default.xml,Hard limit for all replication streams. Hard limit for the number of highest-priority replication streams. ,
Value Relationship (Numeric),"A <= B  , no pattern",no check,N/A,Runtime Failure,None,mapreduce.map.java.opts,mapred-default.xml,mapreduce.map.memory.mb,mapred-default.xml,"mapreduce.map.memory.mb: The amount of memory to request from the scheduler for each map task.
mapreduce.map.java.opts: Java opts only for the child processes that are maps.",
Value Relationship (Numeric),"A’s value(work) is affected by B, and no pattern case",no check,N/A,Performance issues,None,yarn.app.attempt.diagnostics.limit.kc,yarn-default.xml,yarn.resourcemanager.state-store.max-completed-applications,yarn-default.xml," In cases where yarn.resourcemanager.state-store.max-completed-applications is set to a large number, it may be desirable to reduce the value of this property to limit the total data stored. ",
Value Relationship (Set),A is a member of B,Init Time,Init Time Exception,Startup Failure,Partial,dfs.namenode.edits.dir.required,hdfs-default.xml,dfs.namenode.edits.dir,hdfs-default.xml,"This should be a subset of dfs.namenode.edits.dir, to ensure that the transaction (edits) file in these places is always up-to-date. ",
Value Relationship (Set),"A and B’s value should be different , no pattern",no check,N/A,Usability,None,dfs.hosts.exclude,hdfs-default.xml,dfs.hosts,hdfs-default.xml,"dfs.hosts.exclude: Names a file that contains a list of hosts that are not permitted to connect to the namenode.
dfs.hosts: Names a file that contains a list of hosts that are permitted to connect to the namenode.",
Value Relationship (Set),"A and B should be disjointed, no pattern",no check,N/A,Usability,None,yarn.resourcemanager.nodes.exclude-path,yarn-default.xml,yarn.resourcemanager.nodes.include-path,yarn-default.xml,"yarn.resourcemanager.nodes.exclude-path: Path to file with nodes to exclude.
yarn.resourcemanager.nodes.include-path: Path to file with nodes to include.",
Value Relationship (Logical),"If A is set, it requires B to set to a legal value",Init Time,Init Time Exception,Startup Failure,Complete,fs.s3a.proxy.port,core-default.xml,fs.s3a.proxy.host,core-default.xml,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",
Value Relationship (Logical),"If A is set, it requires B to set to a legal value",Init Time,Init Time Exception,Startup Failure,Complete,hadoop.security.dns.nameserver,core-default.xml,hadoop.security.dns.interface,core-default.xml,The host name or IP address of the name server (DNS) which a service Node should use to determine its own host name for Kerberos Login. Requires hadoop.security.dns.interface.,
Value Relationship (Logical),"If A is set, it requires B to set to a legal value",Runtime,Runtime Exception,Runtime Failure,Complete,mapreduce.application.framework.path,mapred-default.xml,mapreduce.application.classpath,mapred-default.xml,"If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath.",
Value Relationship (Logical),"If A is true, it requires B to be true",Init Time,Init Time Exception,Startup Failure,Partial,spark.dynamicAllocation.enabled,Spark.xml,spark.shuffle.service.enabled,Spark.xml,"This must be enabled if spark.dynamicAllocation.enabled is ""true"".",
Value Relationship (Logical),"If A is set, it requires B to set to a legal value",Init Time,Init Time Exception,Startup Failure,Inadequate,alluxio.worker.data.server.domain.socket.address,Alluxio.xml,alluxio.worker.data.server.domain.socket.as.uuid,Alluxio.xml,"If alluxio.worker.data.server.domain.socket.as.uuid is set, the path should be the home directory for the domain socket.",
Value Relationship (Logical),"If A is set, it requires B to set to a legal value",Init Time,Init Time Exception,Startup Failure,Partial,mapreduce.jobhistory.recovery.enable,mapred-default.xml,mapreduce.jobhistory.recovery.store.class,mapred-default.xml,If enabled then mapreduce.jobhistory.recovery.store.class must be specified.,
Value Relationship (Logical),"If A is true, it requires B to be an empty value",Init Time,Init Time Exception,Startup Failure,Complete,hbase.regionserver.hostname.disable.master.reversedns,hbase-default.xml,hbase.regionserver.hostname,hbase-default.xml,Note that this config and hbase.regionserver.hostname are mutually exclusive.,
Value Relationship (Logical),"If A is true, it requires B >= 0",Runtime,Runtime Exception,Runtime Failure,Complete,spark.memory.offHeap.enabled,Spark.xml,spark.memory.offHeap.size,Spark.xml,"If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory use is enabled, then spark.memory.offHeap.size must be positive. ",
Value Relationship (Logical),"if A is ""SSE_C"", B should not be null",Init Time,Init Time Exception,Startup Failure,Partial,fs.s3a.server-side-encryption-algorithm,core-default.xml,fs.s3a.server-side-encryption-key,core-default.xml,Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'.,
Value Relationship (Logical),"If A is set to SomeValue, it requires B to set to a legal value, (isame as #23 in cDep )",Init Time,Init Time Exception,Startup Failure,Inadequate,electionAlg,ZooKeeper.xml,server.x,ZooKeeper.xml,"The leader election port is only necessary if electionAlg is 1, 2, or 3 (default). If electionAlg is 0, then the second port is not necessary.",
Overwrite Dependency ,A will override B,no check,N/A,Usability,None,yarn.app.mapreduce.am.env,mapred-default.xml,yarn.app.mapreduce.am.admin.user.env,mapred-default.xml, These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env),
Overwrite Dependency ,A will override B,no check,N/A,Usability,None,yarn.app.mapreduce.am.command-opts,mapred-default.xml,yarn.app.mapreduce.am.admin-command-opts,mapred-default.xml,It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user.,
Overwrite Dependency ,"if A is set, it will override B",no check,N/A,Usability,None,mapreduce.job.log4j-properties-file,mapred-default.xml,mapreduce.map.log.level,mapred-default.xml,"The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",
Overwrite Dependency ,"if A is set, it will override B",no check,N/A,Usability,None,mapreduce.job.log4j-properties-file,mapred-default.xml,yarn.app.mapreduce.am.log.level,mapred-default.xml,"The setting here could be overriden if ""mapreduce.job.log4j-properties-file"" is set.",
Overwrite Dependency ,"if A is set, it will override B",no check,N/A,Usability,None,mapreduce.job.log4j-properties-file,mapred-default.xml,mapreduce.reduce.log.level,mapred-default.xml,"The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.userbase,core-default.xml,hadoop.security.group.mapping.ldap.base,core-default.xml,"If not set, hadoop.security.group.mapping.ldap.base is used.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,hadoop.security.group.mapping.ldap.groupbase,core-default.xml,hadoop.security.group.mapping.ldap.base,core-default.xml,"If not set, hadoop.security.group.mapping.ldap.base is used.",
Default Value Dependency ,A's default value is B * 10,no check,N/A,Usability,None,dfs.client.read.prefetch.size,hdfs-default.xml,dfs.blocksize,hdfs-default.xml,Defaults to 10 * ${dfs.blocksize}.,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,dfs.namenode.edits.dir,hdfs-default.xml,dfs.namenode.name.dir,hdfs-default.xml,Default value is same as dfs.namenode.name.dir ,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,fs.defaultFS,hdfs-default.xml,dfs.namenode.rpc-address,hdfs-default.xml,If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default. ,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,dfs.internal.nameservices,hdfs-default.xml,dfs.nameservices,hdfs-default.xml,By default this is set to the value of dfs.nameservices. ,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,dfs.journalnode.kerberos.internal.spnego.principal,hdfs-default.xml,dfs.web.authentication.kerberos.keytab,hdfs-default.xml," If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.client.failover-max-attempts,yarn-default.xml,yarn.resourcemanager.connect.max-wait.ms,yarn-default.xml,"When not set, this is inferred from yarn.resourcemanager.connect.max-wait.ms.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.nodemanager.container-monitor.resource-calculator.class,yarn-default.xml,yarn.nodemanager.resource-calculator.class,yarn-default.xml,"If not set, the value for yarn.nodemanager.resource-calculator.class will be used.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn-default.xml,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,yarn-default.xml,"Note that if its value is more than yarn.nodemanager.disk-health-checker. max-disk-utilization-per-disk-percentage or not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.nodemanager.resourcemanager.connect.retry-interval.ms,yarn-default.xml,yarn.resourcemanager.connect.retry-interval.ms,yarn-default.xml,"When not set, proxy will fall back to use value of yarn.resourcemanager.connect.retry-interval.ms. ",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.client.failover-sleep-max-ms,yarn-default.xml,yarn.resourcemanager.connect.retry-interval.ms,yarn-default.xml,"When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.resourcemanager.fail-fast,yarn-default.xml,yarn.fail-fast,yarn-default.xml,"By defalt, it points to ${yarn.fail-fast}.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.nodemanager.resourcemanager.connect.max-wait.ms,yarn-default.xml,yarn.resourcemanager.connect.max-wait.ms,yarn-default.xml,"When not set, proxy will fall back to use value of yarn.resourcemanager.connect.max-wait.ms.",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.nodemanager.container-monitor.interval-ms,yarn-default.xml,yarn.nodemanager.resource-monitor.interval-ms,yarn-default.xml,"If not set, the value for yarn.nodemanager.resource-monitor.interval-ms will be used. ",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,yarn.client.failover-sleep-base-ms,yarn-default.xml,yarn.resourcemanager.connect.retry-interval.ms,yarn-default.xml,"When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead. ",
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,mapreduce.task.profile.map.params,mapred-default.xml,mapreduce.task.profile.params,mapred-default.xml,mapreduce.task.profile.map.params = ${mapreduce.task.profile.params},
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,mapreduce.task.profile.reduce.params,mapred-default.xml,mapreduce.task.profile.params,mapred-default.xml,mapreduce.task.profile.reduce.params = ${mapreduce.task.profile.params},
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,spark.pyspark.driver.python,Spark.xml,spark.pyspark.python,Spark.xml,Python binary executable to use for PySpark in driver. (default is spark.pyspark.python),
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,spark.rpc.askTimeout,Spark.xml,spark.network.timeout,Spark.xml,This config will be used in place of spark.rpc.askTimeout if they are not configured. ,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,spark.rpc.lookupTimeout,Spark.xml,spark.network.timeout,Spark.xml,  This config will be used in place of spark.rpc.lookupTimeout if they are not configured. ,
Default Value Dependency ,A's default value is B * 2,no check,N/A,Usability,None,minSessionTimeout,ZooKeeper.xml,tickTime,ZooKeeper.xml,Defaults to 2 times the tickTime.,
Default Value Dependency ,A's default value is B * 20,no check,N/A,Usability,None,maxSessionTimeout,ZooKeeper.xml,tickTime,ZooKeeper.xml,Defaults to 20 times the tickTime.,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,hbase.hstore.compaction.min,hbase-default.xml,hbase.hstore.compactionThreshold,hbase-default.xml,"In previous versions of HBase, the parameter hbase.hstore.compaction.min was named hbase.hstore.compactionThreshold.",
Default Value Dependency ,A's Default Value is B * 0.1,no check,N/A,Usability,None,spark.executor.memoryOverhead,Spark.xml,spark.executor.memory,Spark.xml,executorMemory * 0.10,
Default Value Dependency ,A's Default Value is B,no check,N/A,Usability,None,spark.driver.bindAddress,Spark.xml,spark.driver.host,Spark.xml,(value of spark.driver.host),
Default Value Dependency ,A's Default Value is B * 0.1,no check,N/A,Usability,None,spark.driver.memoryOverhead,Spark.xml,spark.driver.memory,Spark.xml,driverMemory * 0.10,
Default Value Dependency ,A’s default value is B * C * 2,no check,N/A,Usability,None,hbase.regionserver.thread.compaction.throttle,hbase-default.xml, hbase.hstore.compaction.max,hbase-default.xml,Default: 2 x hbase.hstore.compaction.max x hbase.hregion.memstore.flush.size (which defaults to 128MB).,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,dfs.namenode.checkpoint.edits.dir,hdfs-default.xml,dfs.namenode.checkpoint.dir,hdfs-default.xml,Default value is same as dfs.namenode.checkpoint.dir ,
Default Value Dependency ,A's default value is B,no check,N/A,Usability,None,dfs.namenode.kerberos.internal.spnego.principal,hdfs-default.xml,dfs.web.authentication.kerberos.principal,hdfs-default.xml,In most secure clusters this setting is also used to specify the values for dfs.namenode.kerberos.internal.spnego.principal and dfs.journalnode.kerberos.internal.spnego.principal.,
Default Value Dependency ,A’s default value is  B,no check,N/A,Usability,None,yarn.resourcemanager.state-store.max-completed-applications,yarn-default.xml,yarn.resourcemanager.max-completed-applications,yarn-default.xml,"The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. ",
Default Value Dependency ,A’s default value is affected by B,no check,N/A,Usability,None,alluxio.metrics.conf.file,Alluxio.xml,alluxio.conf.dir,Alluxio.xml,The file path of the metrics system configuration file. By default it is `metrics.properties` in the `conf` directory.,
Behavioral Dependency,the value of A and B are used together to compute to formula,no check,N/A,Usability,None,spark.storage.safetyFraction,Spark.xml,spark.storage.memoryFraction,Spark.xml,Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory.,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,yarn.app.mapreduce.am.container.log.limit.kb,mapred-default.xml,yarn.app.mapreduce.am.container.log.backups,mapred-default.xml,CRLA is enabled for the ApplicationMaster when both yarn.app.mapreduce.am.container.log.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,dfs.namenode.edit.log.autoroll.multiplier.threshold,hdfs-default.xml,dfs.namenode.checkpoint.txns,hdfs-default.xml,The actual threshold (in number of edits) is determined by multiplying this value by dfs.namenode.checkpoint.txns.,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,yarn.app.mapreduce.shuffle.log.backups,mapred-default.xml,yarn.app.mapreduce.shuffle.log.limit.kb,mapred-default.xml,If yarn.app.mapreduce.shuffle.log.limit.kb and yarn.app.mapreduce.shuffle.log.backups are greater than zero then a ContainerRollngLogAppender is used instead of ContainerLogAppender for syslog.shuffle. ,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,dfs.namenode.avoid.write.stale.datanode,hdfs-default.xml,dfs.namenode.write.stale.datanode.ratio,hdfs-default.xml,Writes will avoid using stale datanodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as  stale.,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,dfs.namenode.stale.datanode.interval,hdfs-default.xml,dfs.namenode.avoid.read.stale.datanode,hdfs-default.xml,It can be conditionally avoided for reads (see dfs.namenode.avoid.read.stale.datanode),
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,hbase.zookeeper.quorum,hbase-default.xml,hbase.zookeeper.property.clientPort,hbase-default.xml,"Client-side, we will take this list of ensemble members and put it together with the hbase.zookeeper.property.clientPort config. and pass it into zookeeper constructor as the connectString parameter.",
Behavioral Dependency,A and B work together.,no check,N/A,Usability,None,fs.ftp.host,core-default.xml,fs.ftp.host.port,core-default.xml,FTP filesystem connects to fs.ftp.host on this port,
Behavioral Dependency,A and B work together.,no check,N/A,Usability,None,hbase.hregion.majorcompaction,hbase-default.xml,hbase.hregion.majorcompaction.jitter,hbase-default.xml,A multiplier applied to hbase.hregion.majorcompaction to cause compaction to occur a given amount of time either side of hbase.hregion.majorcompaction.,
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,yarn.nodemanager.remote-app-log-dir-suffix,yarn-default.xml,yarn.nodemanager.remote-app-log-dir,yarn-default.xml, The remote log dir will be created at  {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam},
Behavioral Dependency,A and B work together,no check,N/A,Usability,None,yarn.timeline-service.enabled,yarn-default.xml,yarn.timeline-service.version,yarn-default.xml,"Indicate what is the current version of the running timeline service. For example, if ""yarn.timeline-service.version"" is 1.5, and ""yarn.timeline-service.enabled"" is true, it means the cluster will and should bring up the timeline service",
